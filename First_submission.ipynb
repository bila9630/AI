{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f783779",
   "metadata": {},
   "source": [
    "# 1. Submission\n",
    "student_id: s201687@student.dhbw-mannheim.de\n",
    "<br>This submission is to fulfill the second learning objective. The notebook consists of two parts:\n",
    "<br> **first part**: Implementation of PID to OpenAI Gym Environment (Cartpole)\n",
    "<br> **second part**: Implementation of a Q-Table to the frozen lake game\n",
    "<br> In this notebook, I aim to provide comprehensive explanations of the concept by adding an extensive amount of comments.\n",
    "<br> Why are there two parts? To fulfill the requirement of 200 - 400 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f775ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0346247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment to install the gym package\n",
    "# !pip install gymnasium\n",
    "# required to display\n",
    "# !pip install \"gymnasium[toy-text]\"\n",
    "# !pip install \"gymnasium[classic_control]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb2d1f",
   "metadata": {},
   "source": [
    "## First part: PID on Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3eb831",
   "metadata": {},
   "source": [
    "#### What is a PID?\n",
    "A PID controller is a feedback control loop to regulate a system's behavior. PID stands for \"proportional-integral-derivative,\" which refers to the three components that make up the controller.\n",
    "\n",
    "**Proportional (P)**: This component of the controller produces an output that is proportional to the current error signal. The error signal is the difference between the desired setpoint and the current measured value of the system output. The P component helps to reduce the error signal by producing a corrective action that is proportional to the current error.\n",
    "\n",
    "**Integral (I)**: This component of the controller produces an output that is proportional to the accumulated error signal over time. The I component helps to eliminate steady-state errors by continuously adjusting the control signal based on the system's history of errors.\n",
    "\n",
    "**Derivative (D)**: This component of the controller produces an output that is proportional to the rate of change of the error signal. The D component helps to dampen the system's response to sudden changes in the error signal and reduce overshoot and oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ad2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following cell to play Cartpole as human\n",
    "# a = left, d = right\n",
    "keys = { \"a\": 0, \"d\": 1 }\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# noop=0 means default action is move left\n",
    "play(env, keys_to_action=keys, noop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27ea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change render_mode to \"human\" if you want to see the agent playing\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4976f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PD:\n",
    "    def __init__(self, kp, kd, goal):\n",
    "        self.kp = kp\n",
    "        self.kd = kd\n",
    "        self.goal = goal\n",
    "        self.last_error = 0\n",
    "\n",
    "    def observe(self, x):\n",
    "        error = self.goal - x\n",
    "        d_error = error - self.last_error\n",
    "        self.last_error = error\n",
    "        return self.kp * error + self.kd * d_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d4369",
   "metadata": {},
   "source": [
    "#### What are kp and kd?\n",
    "The kp value determines the strength of the proportional term in the controller. In this case, it is set to 5. A higher kp value would make the controller respond more strongly to errors (deviations from the goal) and could result in faster but more oscillatory control.\n",
    "\n",
    "The kd value determines the strength of the derivative term in the controller. In this case, it is set to 100. The derivative term helps to dampen the control response by taking into account the rate of change of the error. A higher kd value would make the controller respond more strongly to changes in error, which could result in faster but more overshoot-prone control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3537b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = PD(kp=5, kd=100, goal=0)\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Run the game for 1000 episode\n",
    "for episode in range(1000):\n",
    "    # Get the pole angle from the current observation\n",
    "    pole_angle = observation[2]\n",
    "    # Use the PD controller to calculate the control output\n",
    "    control_output = controller.observe(pole_angle)\n",
    "    # Determine the action based on the control output\n",
    "    action = 1 if control_output < 0 else 0\n",
    "    \n",
    "    # Take the action in the environment and get the new observation, reward, termination, and info values\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the game has ended, reset the environment to start a new game\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae900c8",
   "metadata": {},
   "source": [
    "## Second part: Frozen Lake game\n",
    "In the FrozenLake game, the goal of the agent is to navigate a grid of frozen lake tiles, represented as a two-dimensional array of S, F, H, and G characters, where \n",
    "<br>S = Starting point\n",
    "<br>F = Frozen titles\n",
    "<br>H = Holes that the agent must avoid\n",
    "<br>G = Goal\n",
    "<br>The agent can take four possible actions in each state: move up, move down, move left, or move right. The agent receives a reward of 1 for reaching the goal, a reward of 0 for stepping on a frozen tile, and a reward of -1 for falling into a hole. The episode ends when the agent reaches the goal or falls into a hole.\n",
    "<br>For more resource: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36faad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the OpenAI Gym environment \"FrozenLake-v1\"\n",
    "# map_name => map will be a 4x4 grid\n",
    "# is_slippery = True => Move in intended direction with a probability of 1/3\n",
    "# change render_mode to \"human\" if you want to see the agent plays\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90488ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# number of columns\n",
    "action_space_size = env.action_space.n\n",
    "\n",
    "# number of rows\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "# Creating a Q-table with (column, rows) and initializing all values to 0\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c2e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# 16 because there are 16 cells (4x4)\n",
    "# 4 because there are 4 actions (left, right, up, down)\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129123c",
   "metadata": {},
   "source": [
    "#### What is a Q-Table?\n",
    "A Q-table is a table used in reinforcement learning to represent a policy in tabular form. It maps a state-action pair to an expected cumulative reward, which is the expected cumulative reward that an agent will receive if it takes the specified action in the given state and then follows its policy to select actions thereafter. Over time, the Q-table converges to the optimal values, indicating the optimal policy for the agent to follow. This technique is applicable for our use case but is infeasible for larger and more complex environments where the number of possible states and actions is very large or infinite. More sophisticated methods like function approximation or deep learning are used to represent the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d89992",
   "metadata": {},
   "source": [
    "#### How does Q-Table apply to our game?\n",
    "The Q-table for the FrozenLake game has dimensions (number of states, number of actions) and is initialized with all values set to 0. During training, the agent uses an exploration-exploitation strategy to select actions. The Q-values in the table are updated after each action using the Bellman equation, which incorporates the reward received, the Q-value of the next state, and a discount factor that determines the importance of future rewards.\n",
    "\n",
    "Once the agent has been trained, it can use the Q-table to select actions in a greedy manner, choosing the action with the highest Q-value in each state. The performance of the agent is typically evaluated by measuring the percentage of successful episodes, or the average cumulative reward over a set of episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d6607",
   "metadata": {},
   "source": [
    "#### What is an exploration-exploitation strategy?\n",
    "Exploration-exploitation is a strategy that refers to the balance between choosing actions that are already known to be good (exploitation) and trying out new actions to gain more information about the environment (exploration).\n",
    "<br>Initially the agent has limited information about the environment, and it needs to explore to learn which actions lead to higher rewards. Once the agent has gained some knowledge about the environment, it can use this knowledge to exploit the actions that are already known to be good. Essentially it's a trade-off. If the agent only exploits its current knowledge, it may miss better actions that it has not yet tried. On the other hand, if the agent only explores new actions, it may waste time and miss out on immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902424b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate  = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "# 1 -> agent should explore first\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d795d8",
   "metadata": {},
   "source": [
    "- **num_episodes**: An episode in FrozenLake is a playthrough of the game starting from initial state (the starting point of the agent on the frozen lake) and ending when the agent reaches a terminal state (the goal tile or a hole)\n",
    "- **max_steps_per_episode**: Maximum number of steps during an episode\n",
    "- **learning_rate**: Typical learning rate in ml, it controls how much the agent updates its estimates of the values of state-action pairs based on the rewards it receives. A high learning rate means that the agent gives more weight to the most recent rewards when updating its estimates, while a low learning rate means that the agent relies more on past rewards.\n",
    "- **discount_rate**: controls how much the agent values future rewards compared to immediate rewards. Specifically, the discount rate determines the factor by which future rewards are discounted relative to immediate rewards. A high discount rate means that the agent values future rewards more highly than immediate rewards, while a low discount rate means that the agent values future rewards less. The value ranges between 0 to 1, so in our case 0.99 is a pretty high discount rate\n",
    "- **exploration_rate**: controls the trade-off between exploration and exploitation in the agent's behavior. In other words, the exploration rate determines how much the agent should explore the environment (i.e., try new actions) versus exploiting what it has already learned (i.e., selecting actions that it knows lead to high rewards).\n",
    "- **exploration_decay_rate**: ensures that the exploration rate decreases slowly over time, allowing the agent to gradually shift from exploration to exploitation as it learns more about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7facec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Average reward per thousand episodes**\n",
      "\n",
      "count : 0.11500000000000009\n",
      "count : 0.47300000000000036\n",
      "count : 0.6490000000000005\n",
      "count : 0.6880000000000005\n",
      "count : 0.6750000000000005\n",
      "count : 0.6980000000000005\n",
      "count : 0.6560000000000005\n",
      "count : 0.6700000000000005\n",
      "count : 0.6960000000000005\n",
      "count : 0.6680000000000005\n",
      "\n",
      "\n",
      "**Q-Table**\n",
      "\n",
      "[[0.55595145 0.47183209 0.48521633 0.50071495]\n",
      " [0.2584281  0.26511069 0.18060026 0.45908729]\n",
      " [0.3809588  0.25326703 0.20935374 0.25136617]\n",
      " [0.20629013 0.00410903 0.00130697 0.03373852]\n",
      " [0.57179473 0.28503557 0.36373593 0.39380625]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17581237 0.11501583 0.29225537 0.11868138]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.31512028 0.40848283 0.45363947 0.60533011]\n",
      " [0.43986605 0.66057858 0.52260562 0.31519539]\n",
      " [0.65670751 0.42471528 0.45936102 0.32433251]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5189993  0.50365942 0.77609729 0.44860942]\n",
      " [0.70945535 0.92530214 0.75712567 0.72924167]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# empty list to store the rewards for each episode\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # reset environment\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    # set done to False indicating the episode is not over yet\n",
    "    done = False\n",
    "    # reset total reward for current episode\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        # generate a random number between 0 and 1\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        # if the random number is greater than exploration rate\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Exploit and select the action with the highest Q-value for current state\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            # Explore and select a random action\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table for Q(s, a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[obs, :]))\n",
    "        \n",
    "        # Update state\n",
    "        state = obs\n",
    "        # Add reward to current episode\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    # exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"**Average reward per thousand episodes**\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(f\"count : {str(sum(r/1000))}\")\n",
    "    count += 1000\n",
    "\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n**Q-Table**\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f88a1f",
   "metadata": {},
   "source": [
    "The reward for each episode is either 1 or 0. So if we look at the number, for the first 1000 episodes, the agent was winning only 12% of the time. By the last thousand episodes, it was winning 67% all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a637c0",
   "metadata": {},
   "source": [
    "obs, reward, done, truncated, info = env.step(action)\n",
    "<br> example output: (4, 0.0, False, False, {'prob': 0.3333333333333333})\n",
    "- **obs**: The observation/state after the action is taken\n",
    "- **reward**: The reward obtained for taking the action.\n",
    "- **done**: It is a boolean value that indicates whether the episode is terminated or not. If it is True, then the current episode is terminated.\n",
    "- **truncated**: It is a boolean value that indicates whether the episode was truncated due to reaching the maximum number of steps or not.\n",
    "- **info**: Additional information about the environment's state. It is usually not used in the learning algorithm.\n",
    "\n",
    "I add more comment to this line because there are are differences between the old and new version of gym.\n",
    "source: https://stackoverflow.com/questions/52950547/getting-error-valueerror-too-many-values-to-unpack-expected-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d65029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You reached the goal!\n"
     ]
    }
   ],
   "source": [
    "# define env with render_mode \"human\" to display agent playing\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"human\")\n",
    "\n",
    "# Loop through episodes\n",
    "for episode in range(2):\n",
    "    # Reset environment\n",
    "    state = env.reset()[0]\n",
    "    print(f\"EPISODE {episode + 1}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Reset episode-specific variables\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    # Loop through steps in the episode\n",
    "    while not done and step < max_steps_per_episode:\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # Choose action based on Q-table\n",
    "        action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Take action and observe new state and reward\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Update state\n",
    "        state = obs\n",
    "\n",
    "        # Increment step counter\n",
    "        step += 1\n",
    "\n",
    "    # Print final message and pause before moving on to next episode\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    if reward == 1:\n",
    "        print(\"You reached the goal!\")\n",
    "    else:\n",
    "        print(\"You fell through the hole!\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786afea",
   "metadata": {},
   "source": [
    "If it doesn't get displayed, check this thread: https://github.com/openai/gym/issues/762\n",
    "<br> or try this: https://pypi.org/project/gym-notebook-wrapper/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6095a",
   "metadata": {},
   "source": [
    "ressource to create the pid: https://statusfailed.com/blog/2022-12-08-pid-controller-in-the-openai-gym/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cc11f",
   "metadata": {},
   "source": [
    "The creation of the Frozenlake game was created with the help from reinforcement learning playlist from deeplizard: https://youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "941331122c4c5b0e9a76ba712854f74fda5ba1e38e6c463c3ac8fc3ab668796c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
